{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook contains the code to reproduce the experiment in `rank_eval` paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pytrec_eval\n",
    "from tabulate import tabulate\n",
    "\n",
    "from rank_eval import evaluate, Qrels, Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapper for pytrec_eval\n",
    "def run_trec_metrics(qrels, run, metrics):\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics)\n",
    "    results = evaluator.evaluate(run)\n",
    "    return {m: np.mean([v[m] for v in results.values()]) for m in list(metrics)}\n",
    "\n",
    "\n",
    "def run_single_trec_metric(qrels, run, metric):\n",
    "    return run_trec_metrics(qrels, run, {metric})[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_qrels(query_count, max_relevant_per_query):\n",
    "    qrels = {}\n",
    "    for i in range(query_count):\n",
    "        y_t = {}\n",
    "        k = random.choice(range(1, max_relevant_per_query))\n",
    "        for j in range(k):\n",
    "            y_t[f\"d{j}\"] = random.choice([1, 2, 3, 4, 5])\n",
    "\n",
    "        qrels[f\"q{i}\"] = y_t\n",
    "\n",
    "    return qrels\n",
    "\n",
    "\n",
    "def generate_run(query_count, result_count):\n",
    "    run = {}\n",
    "    for i in range(query_count):\n",
    "        y_p = {}\n",
    "        for j in range(result_count):\n",
    "            y_p[f\"d{j}\"] = random.uniform(0.0, 1.0)\n",
    "\n",
    "        run[f\"q{i}\"] = y_p\n",
    "\n",
    "    return run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed = 42\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: 1000000\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "result_count = 100\n",
    "max_relevant_per_query = 10\n",
    "\n",
    "for query_count in [1_000_000]:\n",
    "# for query_count in [1_000, 10_000, 100_000]:\n",
    "    print(f\"Queries: {query_count}\")\n",
    "    # Generate Qrels and Run\n",
    "    trec_qrels = generate_qrels(query_count, max_relevant_per_query)\n",
    "    trec_run = generate_run(query_count, result_count)\n",
    "\n",
    "    re_qrels = Qrels.from_dict(trec_qrels).to_typed_list()\n",
    "    re_run = Run.from_dict(trec_run).to_typed_list()\n",
    "\n",
    "    x = %timeit -o -q run_single_trec_metric(trec_qrels, trec_run, \"map\")\n",
    "    map_avg_time = round(x.average, 3) * 1000\n",
    "    results.append([query_count, \"pytrec_eval\", 1, \"map\", map_avg_time, 1.0])\n",
    "\n",
    "    x = %timeit -o -q run_single_trec_metric(trec_qrels, trec_run, \"recip_rank\")\n",
    "    mrr_avg_time = round(x.average, 3) * 1000\n",
    "    results.append([query_count, \"pytrec_eval\", 1, \"mrr\", mrr_avg_time, 1.0])\n",
    "\n",
    "    x = %timeit -o -q run_single_trec_metric(trec_qrels, trec_run, \"ndcg\")\n",
    "    ndcg_avg_time = round(x.average, 3) * 1000\n",
    "    results.append([query_count, \"pytrec_eval\", 1, \"ndcg\", ndcg_avg_time, 1.0])\n",
    "\n",
    "    for threads in [1, 2, 4, 8]:\n",
    "        # Run metrics once to ensure they have been compiled\n",
    "        evaluate(re_qrels, re_run, [f\"map\", f\"mrr\", f\"ndcg\"], threads=threads)\n",
    "\n",
    "        x = %timeit -o -q evaluate(re_qrels, re_run, f\"map\", threads=threads)\n",
    "        avg_time = max(round(x.average, 3) * 1000, 1)\n",
    "        results.append([query_count, \"rank_eval\", threads, \"map\", avg_time, round(map_avg_time / avg_time, 1)])\n",
    "\n",
    "        x = %timeit -o -q evaluate(re_qrels, re_run, f\"mrr\", threads=threads)\n",
    "        avg_time = max(round(x.average, 3) * 1000, 1)\n",
    "        results.append([query_count, \"rank_eval\", threads, \"mrr\", avg_time, round(mrr_avg_time / avg_time, 1)])\n",
    "\n",
    "        x = %timeit -o -q evaluate(re_qrels, re_run, f\"ndcg\", threads=threads)\n",
    "        avg_time = max(round(x.average, 3) * 1000, 1)\n",
    "        results.append([query_count, \"rank_eval\", threads, \"ndcg\", avg_time, round(ndcg_avg_time / avg_time, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Query count  Approach       Threads  Metric      Avg. (ms)    Speed-Up\n",
      "-------------  -----------  ---------  --------  -----------  ----------\n",
      "      1000000  pytrec_eval          1  map             29452         1\n",
      "      1000000  pytrec_eval          1  mrr             29227         1\n",
      "      1000000  pytrec_eval          1  ndcg            29659         1\n",
      "      1000000  rank_eval            1  map              2019        14.6\n",
      "      1000000  rank_eval            1  mrr               752        38.9\n",
      "      1000000  rank_eval            1  ndcg             3357         8.8\n",
      "      1000000  rank_eval            2  map              1226        24\n",
      "      1000000  rank_eval            2  mrr               562        52\n",
      "      1000000  rank_eval            2  ndcg             2299        12.9\n",
      "      1000000  rank_eval            4  map               875        33.7\n",
      "      1000000  rank_eval            4  mrr               442        66.1\n",
      "      1000000  rank_eval            4  ndcg             1759        16.9\n",
      "      1000000  rank_eval            8  map               683        43.1\n",
      "      1000000  rank_eval            8  mrr               368        79.4\n",
      "      1000000  rank_eval            8  ndcg             1471        20.2\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(results, headers=[\"Query count\", \"Approach\", \"Threads\", \"Metric\", \"Avg. (ms)\", \"Speed-Up\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b69966b56ec652657ae3b55d224973441ad69f336f84cdf432e04c6fe4732776"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('re_dev': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
